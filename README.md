# LabTask1
## Task: 
Составить список всех n-грамм токенов из текста, длиной не больше заданного пользователем значения (то есть, всех последовательностей из 1, 2, 3, …, k слов текста). Организовать их хранение в виде одного префиксного дерева. Найти все фразы длины n, которые составляют больше половины случаев фраз, которые длиннее на одно слово.

Я позволил себе добавить дополнительный параметр *margin*, отвечающий за количество употреблений фраз длины n-1, чтобы конечный результат не наводнялся уникальными или редкими фразами.
